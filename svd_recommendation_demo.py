from pyspark.sql import SparkSession
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col
from pyspark.mllib.recommendation import Rating, MatrixFactorizationModel, ALS as SVD_ALS
import time

# âœ… 1. åˆ›å»º SparkSession å¹¶è¿æ¥ HDFS
spark = SparkSession.builder \
    .appName("SVD_ECommerce_Recommendation") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

sc = spark.sparkContext

# âœ… 2. åŠ è½½è¯„åˆ†æ•°æ®
ratings_df = spark.read.csv("hdfs://localhost:9000/user/suzy/ratings_large.csv", header=True, inferSchema=True)
ratings_df.printSchema()
ratings_df.show()

# âœ… 3. å¼€å§‹è®¡æ—¶
start_time = time.time()

# âœ… 4. è½¬æ¢ä¸º RDD[Rating]
ratings_rdd = ratings_df.select("userId", "productId", "rating") \
    .rdd.map(lambda row: Rating(int(row["userId"]), int(row["productId"]), float(row["rating"])))

# âœ… 5. æ‹†åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†
training_rdd, test_rdd = ratings_rdd.randomSplit([0.8, 0.2], seed=42)

# âœ… 6. è®­ç»ƒ SVD ALS æ¨¡å‹
model = SVD_ALS.train(training_rdd, rank=10, iterations=10, lambda_=0.1)

# âœ… 7. å¦‚æœæµ‹è¯•é›†ä¸ºç©ºåˆ™ä½¿ç”¨è®­ç»ƒé›†
test_user_product = test_rdd.map(lambda r: (r[0], r[1]))
if test_user_product.isEmpty():  # ğŸ”§ æ·»åŠ çš„å®¹é”™å¤„ç†
    print("â— No test data. Using training set instead.")
    test_user_product = training_rdd.map(lambda r: (r[0], r[1]))
    rates = training_rdd.map(lambda r: ((r[0], r[1]), r[2]))
else:
    rates = test_rdd.map(lambda r: ((r[0], r[1]), r[2]))

# âœ… 8. é¢„æµ‹è¯„åˆ† + è®¡ç®— RMSE
predictions = model.predictAll(test_user_product).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = rates.join(predictions)
mse = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
rmse = mse ** 0.5
print(f"ğŸ“‰ RMSE = {rmse:.3f}")

# âœ… 9. ä¸ºæ¯ä½ç”¨æˆ·æ¨è 3 ä¸ªäº§å“
userRecs = model.recommendProductsForUsers(3)

# âœ… 10. è®¡æ—¶ç»“æŸ
end_time = time.time()
print(f"â±ï¸ SVD æ¨èæµç¨‹è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")

# âœ… 11. å±•å¹³æ¨èç»“æœä¸º DataFrame
flatRecs = userRecs.flatMapValues(lambda recs: recs) \
    .map(lambda x: (x[0], x[1].product, x[1].rating)) \
    .toDF(["userId", "productId", "rating"])
flatRecs = flatRecs.filter("rating > 0")

# âœ… 12. åŠ è½½å•†å“ä¿¡æ¯è¡¨ï¼ˆproductId â†’ productNameï¼‰
productInfo = spark.read.csv("hdfs://localhost:9000/user/suzy/products_large.csv", header=True, inferSchema=True)
finalRecs = flatRecs.join(productInfo, on="productId", how="left")

# âœ… 13. æ˜¾ç¤ºæ¨èç»“æœï¼ˆå«å•†å“åï¼‰
print("ğŸ Final Recommendation Results with Product Names:")
finalRecs.orderBy("userId", "rating", ascending=False).show(truncate=False)

# âœ… 14. ä¿å­˜æ¨èç»“æœåˆ°æœ¬åœ° CSV
finalRecs.coalesce(1).write.mode("overwrite").option("header", True).csv("svd_recommendation_output")

# âœ… 15. ç»“æŸ Spark ä¼šè¯
spark.stop()
