from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col, explode, row_number
from pyspark.sql.window import Window
import time

# åˆå§‹åŒ– SparkSession
spark = SparkSession.builder \
    .appName("RunALSFromMappedData") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:8020") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# å¼€å§‹è®¡æ—¶
start_time = time.time()

# è¯»å–ç”¨æˆ·å’Œå•†å“æ˜ å°„è¡¨
user_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_user_id") \
    .toDF("user_id", "userIndex")
item_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_asin_id") \
    .toDF("asin", "itemIndex")

# è¯»å–è¯„åˆ†æ•°æ®ï¼ˆä½¿ç”¨å°æ•°æ®é›†ï¼‰
ratings = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/cleaned_data_small") \
    .toDF("user_id", "asin", "rating")
ratings = ratings.withColumn("rating", col("rating").cast("float"))

# ç¼“å­˜å¹¶è¾“å‡ºè°ƒè¯•ä¿¡æ¯
ratings.cache()
ratings.printSchema()
ratings.show()

# åŠ å…¥ç´¢å¼•ï¼Œæ„é€  ALS è¾“å…¥æ ¼å¼
indexed = ratings.join(user_ids, on="user_id", how="inner") \
                 .join(item_ids, on="asin", how="inner") \
                 .select(col("userIndex").cast("int"), col("itemIndex").cast("int"), col("rating"))

# æ‹†åˆ†è®­ç»ƒ / æµ‹è¯•é›†
training = indexed.sample(False, 0.8, seed=42)
test = indexed.subtract(training)

# æ„å»ºå¹¶è®­ç»ƒ ALS æ¨¡å‹
als = ALS(
    maxIter=10,
    regParam=0.1,
    rank=10,
    userCol="userIndex",
    itemCol="itemIndex",
    ratingCol="rating",
    coldStartStrategy="drop"
)
model = als.fit(training)

# é¢„æµ‹è¯„åˆ†ï¼šè‹¥ test æ— ç»“æœåˆ™å›é€€ training
predictions = model.transform(test)
if predictions.rdd.isEmpty():
    print("â— No predictions generated. Using training set instead.")
    predictions = model.transform(training)

predictions.show()

# è¯„ä¼° RMSE
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)
rmse = evaluator.evaluate(predictions)
print(f"ğŸ“‰ RMSE = {rmse:.3f}")

# æ¯ä½ç”¨æˆ·æ¨è 3 ä¸ªå•†å“
user_recs = model.recommendForAllUsers(3)
print("ğŸ”® Top 3 Recommendations for Each User:")
user_recs.show(truncate=False)

# æ¨èæµç¨‹è®¡æ—¶ç»“æŸ
end_time = time.time()
print(f"â±ï¸ ALS æ¨èæµç¨‹è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")

# æ‰å¹³åŒ–æ¨èç»“æœ
exploded = user_recs.select("userIndex", explode("recommendations").alias("rec"))
recommendations = exploded.select(
    col("userIndex"),
    col("rec.itemIndex").alias("itemIndex"),
    col("rec.rating").alias("predictedRating")
)

# å»é™¤å·²è¯„åˆ†å•†å“
user_item_df = indexed.select("userIndex", "itemIndex")
final_recommendations = recommendations.join(user_item_df, on=["userIndex", "itemIndex"], how="left_anti")

# æ¯ä½ç”¨æˆ·ä¿ç•™ top 3 æ¨è
windowSpec = Window.partitionBy("userIndex").orderBy(col("predictedRating").desc())
ranked = final_recommendations.withColumn("rank", row_number().over(windowSpec))
topN = ranked.filter(col("rank") <= 3)

# å±•ç¤ºæ¨èç»“æœ
topN.select("userIndex", "itemIndex", "predictedRating").show(truncate=False)

# åœæ­¢ Spark
spark.stop()