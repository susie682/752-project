from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, row_number
from pyspark.sql.window import Window
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.mllib.recommendation import ALS as SVD_ALS, Rating
import time

# åˆå§‹åŒ– SparkSession
spark = SparkSession.builder \
    .appName("RunSVDFromMappedData") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:8020") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

sc = spark.sparkContext

# å¼€å§‹è®¡æ—¶
start_time = time.time()

# è¯»å–ç”¨æˆ·å’Œå•†å“æ˜ å°„è¡¨
user_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_user_id") \
    .toDF("user_id", "userIndex")
item_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_asin_id") \
    .toDF("asin", "itemIndex")

# è¯»å–è¯„åˆ†æ•°æ®ï¼ˆä½¿ç”¨å°æ•°æ®é›†ï¼‰
ratings = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/cleaned_data_small") \
    .toDF("user_id", "asin", "rating")
ratings = ratings.withColumn("rating", col("rating").cast("float"))

# è¿æ¥æ˜ å°„å¾—åˆ°æ•´æ•° ID
indexed = ratings.join(user_ids, on="user_id", how="inner") \
                 .join(item_ids, on="asin", how="inner") \
                 .select(
                     col("userIndex").cast("int").alias("userId"),
                     col("itemIndex").cast("int").alias("productId"),
                     col("rating")
                 )

indexed.cache()
indexed.printSchema()
indexed.show()


ratings_rdd = indexed.rdd.map(lambda row: Rating(row["userId"], row["productId"], row["rating"]))


training_rdd, test_rdd = ratings_rdd.randomSplit([0.8, 0.2], seed=42)


model = SVD_ALS.train(training_rdd, rank=10, iterations=10, lambda_=0.1)

# å›é€€æœºåˆ¶ï¼šè‹¥æµ‹è¯•é›†ä¸ºç©ºåˆ™ä½¿ç”¨è®­ç»ƒé›†
if test_rdd.isEmpty():
    print("â— No test data. Using training set instead.")
    test_user_product = training_rdd.map(lambda r: (r[0], r[1]))
    rates = training_rdd.map(lambda r: ((r[0], r[1]), r[2]))
else:
    test_user_product = test_rdd.map(lambda r: (r[0], r[1]))
    rates = test_rdd.map(lambda r: ((r[0], r[1]), r[2]))

# é¢„æµ‹å¹¶è®¡ç®— RMSE
predictions = model.predictAll(test_user_product).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = rates.join(predictions)
mse = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
rmse = mse ** 0.5
print(f"ğŸ“‰SVD  RMSE = {rmse:.3f}")

# æ¨èæ¯ä½ç”¨æˆ· 3 ä¸ªå•†å“
userRecs = model.recommendProductsForUsers(3)

# è®¡æ—¶ç»“æŸ
end_time = time.time()
print(f"â±ï¸ SVD recommendation time costï¼š{end_time - start_time:.2f} Seconds")

# æ‰å¹³åŒ–æ¨èç»“æœä¸º DataFrame
flatRecs = userRecs.flatMapValues(lambda recs: recs) \
    .map(lambda x: (x[0], x[1].product, x[1].rating)) \
    .toDF(["userId", "productId", "predictedRating"])

# å»é™¤ç”¨æˆ·å·²è¯„åˆ†å•†å“
user_item_df = indexed.select("userId", "productId")
final_recommendations = flatRecs.join(user_item_df, on=["userId", "productId"], how="left_anti")

# æ¯ä½ç”¨æˆ·ä¿ç•™ top 3
windowSpec = Window.partitionBy("userId").orderBy(col("predictedRating").desc())
ranked = final_recommendations.withColumn("rank", row_number().over(windowSpec))
topN = ranked.filter(col("rank") <= 3)

# å±•ç¤ºæ¨èç»“æœ
topN.select("userId", "productId", "predictedRating").show(truncate=False)

# åœæ­¢ Spark
spark.stop()