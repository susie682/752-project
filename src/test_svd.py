from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, row_number
from pyspark.sql.window import Window
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.mllib.recommendation import ALS as SVD_ALS, Rating
import time

# åˆå§‹åŒ– SparkSession
spark = SparkSession.builder \
    .appName("RunSVDFromMappedData") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:8020") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

sc = spark.sparkContext
x = 1000000
# å¼€å§‹è®¡æ—¶


# è¯»å–ç”¨æˆ·å’Œå•†å“æ˜ å°„è¡¨
user_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_user_id") \
    .toDF("user_id", "userIndex")
item_ids = spark.read.option("header", "false").csv("hdfs://localhost:8020/user/ecommerce_project/mapping_asin_id") \
    .toDF("asin", "itemIndex")

# è¯»å–è¯„åˆ†æ•°æ®ï¼ˆä½¿ç”¨å°æ•°æ®é›†ï¼‰
ratings = spark.read.option("header", "false").csv(f"hdfs://localhost:8020/user/ecommerce_project/cleaned_data_{x}") \
    .toDF("user_id", "asin", "rating")
ratings = ratings.withColumn("rating", col("rating").cast("float"))

# è¿æ¥æ˜ å°„å¾—åˆ°æ•´æ•° ID
indexed = ratings.join(user_ids, on="user_id", how="inner") \
                 .join(item_ids, on="asin", how="inner") \
                 .select(
                     col("userIndex").cast("int").alias("userId"),
                     col("itemIndex").cast("int").alias("productId"),
                     col("rating")
                 )

indexed.cache()



ratings_rdd = indexed.rdd.map(lambda row: Rating(row["userId"], row["productId"], row["rating"]))

start_time = time.time()
training_rdd, test_rdd = ratings_rdd.randomSplit([0.9, 0.1], seed=42)


model = SVD_ALS.train(training_rdd, rank=30, iterations=20, lambda_=0.1)

# å›é€€æœºåˆ¶ï¼šè‹¥æµ‹è¯•é›†ä¸ºç©ºåˆ™ä½¿ç”¨è®­ç»ƒé›†
if test_rdd.isEmpty():
    print("â— No test data. Using training set instead.")
    test_user_product = training_rdd.map(lambda r: (r[0], r[1]))
    rates = training_rdd.map(lambda r: ((r[0], r[1]), r[2]))
else:
    test_user_product = test_rdd.map(lambda r: (r[0], r[1]))
    rates = test_rdd.map(lambda r: ((r[0], r[1]), r[2]))

# é¢„æµ‹å¹¶è®¡ç®— RMSE
predictions = model.predictAll(test_user_product).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = rates.join(predictions)
mse = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
rmse = mse ** 0.5
print(f"data count = {x}")
print(f"ğŸ“‰SVD  RMSE = {rmse:.3f}")



# è®¡æ—¶ç»“æŸ
end_time = time.time()
print(f"â±ï¸ SVD recommendation time costï¼š{end_time - start_time:.2f} Seconds")



# åœæ­¢ Spark
spark.stop()