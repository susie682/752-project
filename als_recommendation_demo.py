from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import explode
import time  

# âœ… 1. åˆ›å»º SparkSessionï¼Œå¹¶è¿æ¥ HDFSï¼ˆæ³¨æ„ fs.defaultFS é…ç½®ï¼‰
spark = SparkSession.builder \
    .appName("ALS_ECommerce_Recommendation") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# âœ… 2. ä» HDFS åŠ è½½ CSV æ ¼å¼çš„è¯„åˆ†æ•°æ®ï¼ˆuserId, productId, ratingï¼‰
ratings = spark.read.csv("hdfs://localhost:9000/user/suzy/ratings_large.csv", header=True, inferSchema=True)

# âœ… åŠ è¿™ä¸¤è¡Œï¼šè°ƒè¯•ç”¨ï¼Œæ‰“å°å­—æ®µç»“æ„å’Œå†…å®¹
ratings.printSchema()
ratings.show()

# âœ… 3. ç¼“å­˜æ•°æ®ï¼Œæ–¹ä¾¿åç»­å¤šæ¬¡ä½¿ç”¨
ratings.cache()
ratings.show()


# âœ… åœ¨æ‹†åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†å‰å¼€å§‹è®¡æ—¶
start_time = time.time()

# âœ… 4. æ‹†åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†ï¼Œé¿å… cold start é—®é¢˜
training = ratings.sample(False, 0.8, seed=42)
test = ratings.subtract(training)

# âœ… 5. æ„å»º ALS æ¨¡å‹
als = ALS(
    userCol="userId",
    itemCol="productId",
    ratingCol="rating",
    maxIter=10,
    regParam=0.1,
    rank=10,
    coldStartStrategy="drop"  # é¿å…é¢„æµ‹å€¼ä¸º NaN
)

# âœ… 6. æ‹Ÿåˆæ¨¡å‹
model = als.fit(training)

# âœ… 7. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹è¯„åˆ†
predictions = model.transform(test)

# âœ… 8. è‹¥æ— é¢„æµ‹ç»“æœï¼ˆcold startï¼‰ï¼Œæ”¹ç”¨è®­ç»ƒé›†æµ‹è¯•
if predictions.rdd.isEmpty():
    print("â— No predictions generated. Using training set instead.")
    predictions = model.transform(training)
predictions.show()

# âœ… 9. ä½¿ç”¨ RMSE æŒ‡æ ‡è¯„ä¼°æ¨¡å‹
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)
rmse = evaluator.evaluate(predictions)
print(f"ğŸ“‰ RMSE = {rmse:.3f}")

# âœ… 10. ä¸ºæ¯ä½ç”¨æˆ·æ¨è 3 ä¸ªå•†å“
userRecs = model.recommendForAllUsers(3)
print("ğŸ”® Top 3 Recommendations for Each User:")
userRecs.show(truncate=False)

# âœ… åœ¨ userRecs è¾“å‡ºæ¨èåç»“æŸè®¡æ—¶å¹¶æ‰“å°ç”¨æ—¶
end_time = time.time()
print(f"â±ï¸ ALS æ¨èæµç¨‹è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")

# âœ… 1. æ‰å¹³åŒ–æ¨èç»“æœ
flatRecs = userRecs.withColumn("rec", explode("recommendations")) \
                   .select("userId", "rec.productId", "rec.rating")
                   
# âœ… 2. æ˜¾ç¤ºæ‰å¹³åŒ–åçš„æ¨èç»“æœ
flatRecs = flatRecs.filter("rating > 0")

# âœ… 2. åŠ è½½å•†å“ä¿¡æ¯è¡¨ï¼ˆproductId â†’ productNameï¼‰
productInfo = spark.read.csv("hdfs://localhost:9000/user/suzy/products_large.csv", header=True, inferSchema=True)

# âœ… 3. è”è¡¨ï¼Œå¾—åˆ°å¯è¯»çš„æ¨èç»“æœ
finalRecs = flatRecs.join(productInfo, on="productId", how="left")

# âœ… 4. æ˜¾ç¤ºæ¨èç»“æœï¼ˆå«å•†å“åï¼‰
print("ğŸ Final Recommendation Results with Product Names:")
finalRecs.orderBy("userId", "rating", ascending=False).show(truncate=False)

# âœ… 5. ä¿å­˜æ¨èç»“æœåˆ°æœ¬åœ° CSV
finalRecs.coalesce(1).write.mode("overwrite").option("header", True).csv("recommendation_output")


# âœ… ï¼ˆå¯é€‰ï¼‰ä¿å­˜æ¨èç»“æœåˆ° HDFS
# finalRecs.coalesce(1).write.option("header", True).csv("hdfs://localhost:9000/user/suzy/recommendation_output")


# âœ… 11. ç»“æŸ Spark ä¼šè¯
spark.stop()